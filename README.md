# Neural Network Practicum

Neural Networks have become an indispensbile technology for the machine learning community.  That said, there is a wide gap between those that truly understand how to design and implement neural networks and those that just add more layers with a relu activation and pray to get a good segment of the data to train on.  This book attempts to bridge the gap between knowing the basics of linear algebra, calculus, and some programming to truly understanding how to make use of neural networks and implement them well.  This course of study is inherently advanced, and you should know some linear algebra, calculus and python as well as some algorithms before attempting this book.  I will make no effort to explain any of these concepts in depth.  Instead, I will focus on creating neural networks with custom layers, and analyzing how to use data as a sort of prior to best design and develop such networks.  The intention of this book is to follow the modeling advice of Einstein - "your model ought to be as simple as possible, but no simpler".  With this as my guiding principle, I will attempt to build up a set of analytic tools to help you, the reader analyze and understand neural networks as well as some attempt at understanding modeling in general.  Then we will look at how to design and develop networks with a scientific mindset - through experimentation, scientific inquiry; specifically the notion of forming hypotheses and then checking those hypotheses through experimentation. And then we will build these notions directly into our custom layers.  This book is biased and will use tensorflow 2.x as the basis for our treatment of neural networks.  There are other frameworks that are just as good, or maybe better.  But this is what I know and I'm lazy.  